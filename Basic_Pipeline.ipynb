{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Appearance Models with Menpo for Infrared Images\n",
    "\n",
    "Requirements: menpo 0.7.7 menpofit 0.4.1 menpodetect 0.4.0 (never versions still provide the same functionality, however may provide a different interface, thereby requiring chnges to this code)\n",
    "\n",
    "Download using anaconda or check menpo.org for details\n",
    "\n",
    "### Note that Windows support has been suspended by the menpo developers.\n",
    "We can provide you with a copy of our compatile local windows environment, wich you can use at your own risk. It is in the same cloud folder as the database and this code. Extract it into the ``envs`` subfolder of your conda installation and hope for the best.\n",
    "\n",
    "## Train Your AAM\n",
    "\n",
    "### Settings AAM-Training\n",
    "\n",
    "To train an Active Appearance Model (AAM) a database of images with a consistent number of labeled points per image is required. The path to this database should be specified below in ``IMAGE_PATH``. The labeled points should either be in a ``.pts`` or a ``.ljson`` file and the filetype has to be specified in ``LANDMARK_GROUP``. \n",
    "Afterwards a feature descriptor has to be selected:\n",
    "\n",
    " * ``no_op`` uses the raw image data\n",
    " * ``d_sift`` and ``fast_dsift`` are computing [dense sift features](https://pdfs.semanticscholar.org/ac08/4587faa2227e8e09a0d2b7803f60f23be1c1.pdf)\n",
    " * ``hog`` will compute the [histogram of oriented gradients](http://web.eecs.umich.edu/~silvio/teaching/EECS598_2010/slides/09_28_Grace.pdf)\n",
    "\n",
    "At the end a scale and an image diagonal has to be choosed (recommended to use default parameters at the beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from menpo.feature import hog, ndfeature # you can import no_op and dsift as well (hog is best for imfrared images though)\n",
    "IMAGE_PATH = \"d:/downloads/FaceDB_PNG_2935\" # \"/home/temp/schock/Infrared/Databases/FaceDB_Snapshot\"\n",
    "LANDMARK_GROUP = \"LJSON\" #or \"PTS\"\n",
    "# use hog or dsift if you want precision, no_op to save memory and speed (but no_op performs _really_ bad on thermal data)\n",
    "features = hog\n",
    "\n",
    "# you will need atleast 8, preferably 16 GB RAM to train the model with these settings\n",
    "scales = 1 # use 2 if you have enough RAM (>32 GB)\n",
    "\n",
    "# use 120 or 150 if you have enough RAM - at higher values and with dsift or hog features, this eats RAM like popcorn\n",
    "# larger diagonal = higher precision. Results in the paper were with 150 iirc.\n",
    "diagonal = 50 \n",
    "\n",
    "# convert feature from 64 to 32 bit; has no impact on fitting precision but saves 50% memory\n",
    "# thanks to the menpo team for the hint\n",
    "# you should define the same for your other features if you use other features than hog in your code\n",
    "@ndfeature\n",
    "def float32_hog(x):\n",
    "    return hog(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Images\n",
    "\n",
    "With ``mio.import images`` we firstly import all images their corresponding landmarks in ``IMAGE_PATH``.\n",
    "Afterwards we crop every image to its respective landmarks to ensure that the resulting image contains only the face.\n",
    "The last step of loading the images is to convert them to greyscale if they are not already.\n",
    "> #### Note: The landmark-files must have the same name as the corresponding image-files but with different file-exstensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 3/2935 [00:00<01:43, 28.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2935/2935 [01:18<00:00, 37.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported 2935 Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from menpo import io as mio\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Importing images\")\n",
    "train_images = []\n",
    "\n",
    "for i in tqdm(mio.import_images(IMAGE_PATH)):\n",
    "    \n",
    "    # Crop images to Landmarks --> only Face on resulting image\n",
    "    i = i.crop_to_landmarks_proportion(0.1)\n",
    "    \n",
    "    # Convert multichannel images to greyscale\n",
    "    if i.n_channels > 2:\n",
    "        i = i.as_greyscale()\n",
    "        \n",
    "    train_images.append(i)\n",
    "\n",
    "    \n",
    "print(\"Succesfully imported %d Images\" % len(train_images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the AAM\n",
    "The trainstage of an Active Appearance Model is almost only a PCA for each of the model parts (Shape Model and Appearance Model) and storing their results. Therefore the code for training an AAM is quite simple using the previous defined settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AAM\n",
      "- Computing reference shape                                                     Computing batch 0\n",
      "- Building modelsges size: [==========] 100% (2935/2935) - done.                \n",
      "  - Warping images: [          ] 0% (23/2935) - 00:00:18 remaining              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\menpofit\\builder.py:338: MenpoFitModelBuilderWarning: The reference shape passed is not a TriMesh or subclass and therefore the reference frame (mask) will be calculated via a Delaunay triangulation. This may cause small triangles and thus suboptimal warps.\n",
      "  MenpoFitModelBuilderWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Doneding appearance model                                                   \n",
      "                                                                       "
     ]
    }
   ],
   "source": [
    "from menpofit.aam import HolisticAAM as AAM\n",
    "\n",
    "print(\"Training AAM\")\n",
    "aam = AAM(\n",
    "    train_images,\n",
    "    group=LANDMARK_GROUP,\n",
    "    verbose=True,\n",
    "    holistic_features=features,\n",
    "    scales=scales,\n",
    "    diagonal = diagonal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fitting from a trained AAM\n",
    "### Creating an AAM-Fitter : LucasKanadeFitter\n",
    "To fit images using the trained AAM a fitter is necessary. The fitter is a class cappable of the whole optimization procedure. \n",
    "As compositional gradient descent algorithms either the [Wiberg Inverse Compositional Gauss-Newton algorithm (WIC)](http://menpofit.readthedocs.io/en/stable/api/menpofit/aam/WibergInverseCompositional.html) or the [Simultaneous Inverse Compositional Gauss-Newton algorithm (SIC)](http://menpofit.readthedocs.io/en/stable/api/menpofit/aam/SimultaneousInverseCompositional.html) should be used [1](https://link.springer.com/article/10.1007%2Fs11263-016-0916-3).\n",
    "With the parameters ``n_shape`` and ``n_appearance`` we can specify how many of the PCA-components should be used for the fitting process. Setting them to a float less between zero and one it defines the fraction of accuracy we want to achieve at the certain model. Setting it to an int greater than one it defines the number of components to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Fitter\n"
     ]
    }
   ],
   "source": [
    "from menpofit.aam import LucasKanadeAAMFitter as Fitter\n",
    "from menpofit.aam import WibergInverseCompositional as WIC\n",
    "from menpofit.aam import SimultaneousInverseCompositional as SIC\n",
    "\n",
    "print(\"Creating Fitter\")\n",
    "fitter_alg = WIC # or SIC  --> Algorithm to be used by fitter\n",
    "n_shape = 0.95 #  --> fraction of shape accuracy to remain (dimensionality reduction through PCA)\n",
    "n_appearance = 0.95 #  --> fraction of appearance accuracy to remain (dimensionality reduction through PCA)\n",
    "\n",
    "fitter = Fitter(aam=aam, \n",
    "                lk_algorithm_cls=fitter_alg,\n",
    "                n_shape=n_shape, \n",
    "                n_appearance=n_appearance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection\n",
    "To recieve a good result from our LucasKanadeAAMFitter we need to give it a bounding box of the face as initialization. This bounding box shows defines the position and scale of the AAM's initial shape.\n",
    "\n",
    "To do so, we need to define a function which gets a list of faces and returns the bounding box of the first shape as [PointDirectedGraph](http://docs.menpo.org/en/stable/api/shape/PointDirectedGraph.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import menpo\n",
    "\n",
    "def face_2_pointcloud(faces):\n",
    "    if len(faces):\n",
    "            face = np.array([faces[0].as_vector()[1], faces[0].as_vector()[0],\n",
    "                             faces[0].range()[1], faces[0].range()[0]]).astype(np.uint16)\n",
    "            print(\"Face detected. > \", face)\n",
    "    else:\n",
    "        face = np.array([0, 0, 0, 0]).astype(np.uint16)\n",
    "        print(\"NO Face detected.\")\n",
    "    \n",
    "    points = np.array([[face[1], face[0]],\n",
    "                         [face[1]+face[3], face[0]],\n",
    "                         [face[1]+face[3], face[0]+face[2]],\n",
    "                         [face[1], face[0]+face[2]]])\n",
    "\n",
    "    adjacency_matrix = np.array([[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0]])\n",
    "\n",
    "    return menpo.shape.PointDirectedGraph(points, adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we load a pretrained [hog-facedetector](http://blog.dlib.net/2014/02/dlib-186-released-make-your-own-object.html) to detect the faces and return their bounding boxes:\n",
    "\n",
    "> #### Note: The face detector has been trained on infrared images. To guarantee good performance on other image types it should be retrained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detector\n"
     ]
    }
   ],
   "source": [
    "import menpodetect\n",
    "import dlib\n",
    "HOG_PATH = \"./hog_detector.svm\"\n",
    "\n",
    "print(\"Loading Face Detector\")\n",
    "face_detector = menpodetect.DlibDetector(dlib.simple_object_detector(HOG_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we load the test-image and detect it's face using the loaded face-detector and and the previously written function ``face_2_pointcloud`` will return us a bounding box as PointDirectedGraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Image\n",
      "Detecting Face\n",
      "Face detected. >  [279 229 445 446]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\menpo\\image\\base.py:2652: MenpoDeprecationWarning: This method is no longer supported and will be removed in a future version of Menpo. Use .pixels_with_channels_at_back instead.\n",
      "  MenpoDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "TEST_IMG_PATH = \"D:/downloads/FaceDB_PNG_2935/irface_sub051_seq07_frm01053.jpg_lfb.png\"#\"PATH/TO/TEST_IMG\" # \"/home/temp/schock/Infrared/Databases/IR_HPE_Colette/001/image_00000.png\" \n",
    "\n",
    "print(\"Loading Test Image\")\n",
    "test_img = mio.import_image(TEST_IMG_PATH)\n",
    "\n",
    "print(\"Detecting Face\")\n",
    "test_face_bb = face_2_pointcloud(face_detector(test_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting\n",
    "Using the detected face's bounding box, we fit from the bounding box by invoking the fitters function ``fit_from_bb`` and passing it the loaded image and the bounding box. The argument ``max_iters=25`` defines the maximum number of compositional gradient descent iterations to be 25, which is usually a good tradeoff between performance and accuracy.\n",
    "\n",
    "The achieved ``fitting_result`` can be visualized with ``fitting_result.view()``. To show the created figure the pyplot ``show`` function is necessary afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting AAM\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.colors' has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1d859d4c28d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfitting_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     37\u001b[0m             display(\n\u001b[0;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             )\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# determine if a background is needed for legibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;31m# the background is transparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[1;32mC:\\Miniconda3\\envs\\menpo\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\u001b[0m in \u001b[0;36m_is_transparent\u001b[1;34m(color)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.colors' has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Fitting AAM\")\n",
    "fitting_result = fitter.fit_from_bb(test_img, test_face_bb, max_iters=25)\n",
    "\n",
    "fitting_result.view()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Detection\n",
    "### Loading the pretrained classifier\n",
    "To classify the face's emotion we provide a pretrained classifier. To load this classifier the following function tries to load it using ``joblib`` and switches back to loading it with ``pickle`` for backward compatibility if neccessary :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "def load_model(file_path: str):\n",
    " \n",
    "    try:\n",
    "        model = joblib.load(file_path)\n",
    "    except:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            model = pickle.load(f, encoding='latin1')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier will now be loaded with the above defined function. Therefore the filepath to the classifier file has to be specified in ``EMOTION_CLF_PATH``. For the plotting step we also need to specify the emotions, the classifier is able to predict in the correct order as ``class_labels``:\n",
    "\n",
    "> #### Note: The given emotion classifier has been trained on HoG-features of inrared images. They might be different from HoG-features of other image types. Therefore a retraining of the classifier is necessary to guarantee good performance on other image types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_CLF_PATH = \"./classifier_29092016_neutral_freude_trauer_ueberraschung.pkl\" # \"/PATH/TO/CLF\"\n",
    "class_labels = [\"neutral\", \"joy\", \"sorrow\", \"surprise\"]\n",
    "print(\"Loading Emotion Classifier\")\n",
    "emotion_clf = load_model(EMOTION_CLF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the relevant features\n",
    "#### Specify relevant image part\n",
    "To detect the emotion we use the prediction we obtianed from the AAM or more precisely its boundaries. We extract the points on the upper left and the bottom right of the bounding box and clip their values to the image range.\n",
    "\n",
    "Afterwards we extract the part inside the bounding box and resize it to 144 x 144 pixels because our classifier was trained on HoG-Features of images which were sized like this.\n",
    "\n",
    "The last two lines plot the extracted image part to verify that the right part was extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# get points of bounding_box. left top and bottom right\n",
    "p_left_top = fitting_result.final_shape.bounds()[0]\n",
    "p_right_bottom = fitting_result.final_shape.bounds()[1]\n",
    "\n",
    "image_width, image_height = test_img.width, test_img.height\n",
    "\n",
    "\n",
    "# clip value to image range\n",
    "p_left_top[0] = np.clip([p_left_top[0]], 0, image_height)[0]\n",
    "p_left_top[1] = np.clip([p_left_top[1]], 0, image_width)[0]\n",
    "p_right_bottom[0] = np.clip([p_right_bottom[0]], 0, image_height)[0]\n",
    "p_right_bottom[1] = np.clip([p_right_bottom[1]], 0, image_width)[0]\n",
    "\n",
    "#extract relevant image part\n",
    "img_tmp = test_img.pixels.squeeze()[p_left_top[0]:p_right_bottom[0], p_left_top[1]:p_right_bottom[1]]*255\n",
    "\n",
    "img_tmp = cv2.resize(img_tmp, (144, 144))\n",
    "\n",
    "plt.imshow(img_tmp, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HoG-Features\n",
    "Now, we need to extract the Hog features of our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "hog_features, hog_image = hog(img_tmp, orientations=9, pixels_per_cell=(8, 8),\n",
    "                               cells_per_block=(2, 2), visualise=True)\n",
    "\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "Afterwards we feed our extracted features to our classifier to get probabilites for each emotion and plot then result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_probabilities = np.array(emotion_clf.predict_proba([hog_features])[0])\n",
    "\n",
    "classes = np.arange(len(emotion_probabilities))\n",
    "plt.bar(left=classes, height=emotion_probabilities, align='center', tick_label=class_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
