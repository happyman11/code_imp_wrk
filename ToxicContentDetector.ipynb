{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ToxicContentDetector.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"szMXQg7lfrgZ"},"source":["# Detecting Slang Using BERT"]},{"cell_type":"markdown","metadata":{"id":"l_oT-kssfzYp"},"source":["### Installing Hugging Face library"]},{"cell_type":"code","metadata":{"id":"mCcR-xrEowGa"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"GnQnKCPEUAgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CL0yF_V_odLN"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","from tokenizers import BertWordPieceTokenizer\n","from tqdm.notebook import tqdm\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import backend as K\n","import transformers\n","from transformers import TFAutoModel, AutoTokenizer\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kwN5740o8IS"},"source":["# Downloading data\n"]},{"cell_type":"code","metadata":{"id":"tYXDgobj7JvA"},"source":["!wget https://raw.githubusercontent.com/abcom-mltutorials/detecting-slang/master/jigsawdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0YLT-fW7U6c"},"source":["!unzip '/content/jigsawdata.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gifFeMlqpO3G"},"source":["train = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n","test = pd.read_csv('/content/drive/MyDrive/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11J1VyWXCvp5"},"source":["# Examining data"]},{"cell_type":"code","metadata":{"id":"S_8ze8MtSDlk"},"source":["# train[train['toxic']==1]\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["comments_size=[]\n","for i in train[\"comment_text\"]:\n","   comments_size.append(len(i))\n","\n","import numpy as np\n","comments_size_np=np.asarray(comments_size)\n"],"metadata":{"id":"jR6P1uoeZnCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkdI2cTr4c5N"},"source":["columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n","zeros =[]\n","ones =[]\n","for col in columns:\n","  zeros.append(train[col].value_counts()[0])\n","  ones.append(train[col].value_counts()[1])\n","  \n","df = pd.DataFrame({'zero': zeros,'one': ones}, index=columns)\n","df.plot.bar(rot=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n","labels=[]\n","for i in range(0,159571):\n","  t1=train.toxic[i]\n","  t2=train.severe_toxic[i]\n","  t3=train.obscene[i]\n","  t4=train.threat[i]\n","  t5=train.insult[i]\n","  t6=train.identity_hate[i]\n","\n","  Total=t1+t2+t3+t4+t5+t6\n","  if Total >0:\n","    labels.append(1)\n","  else:\n","    labels.append(0)\n","\n","train[\"labels\"]=labels\n"],"metadata":{"id":"i6kxt7yiT7y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train[\"labels\"].value_counts()\n"],"metadata":{"id":"0vmdW7vKX33h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3D_La4d5nPz"},"source":["### Target distribution"]},{"cell_type":"markdown","metadata":{"id":"CyqdfRXzuCtL"},"source":["# Build the model\n","Instantiating TFAutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant BERT architecture "]},{"cell_type":"code","metadata":{"id":"qoecxsFMyYv8"},"source":["def build_model(transformer, loss, max_len=128):\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    sequence_output = transformer(input_word_ids)[0]\n","    cls_token = sequence_output[:, 0, :]\n","    x = tf.keras.layers.Dropout(0.35)(cls_token)\n","    out = Dense(100, activation='relu')(x)\n","    # out = Dense(75, activation='relu')(out)\n","    # out = tf.keras.layers.Dropout(0.35)(out)\n","    # out = Dense(50, activation='relu')(out)\n","    # out = Dense(25, activation='relu')(out)\n","    # out = tf.keras.layers.Dropout(0.35)(out)\n","    out = Dense(10, activation='relu')(out)\n","    out = Dense(5, activation='relu')(out)\n","    out = Dense(1, activation='sigmoid')(out)\n","    \n","    model = Model(inputs=input_word_ids, outputs=out)\n","    model.compile(Adam(lr=3e-5), loss=loss, metrics=[])\n","    # model.summary()\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXqUoxnR62Nl"},"source":["# The focal loss function"]},{"cell_type":"code","metadata":{"id":"25W6o-cBpIu6"},"source":["def focal_loss(gamma=2., alpha=.2):\n","    def focal_loss_fixed(y_true, y_pred):\n","        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n","        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n","        return -K.mean(alpha * \n","                       K.pow(1. - pt_1, gamma) * \n","                       K.log(pt_1)) - K.mean((1 - alpha) * \n","                       K.pow(pt_0, gamma) * \n","                       K.log(1. - pt_0))\n","    return focal_loss_fixed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7Lhfo1w65P-"},"source":["# Instantiating model"]},{"cell_type":"code","metadata":{"id":"eByghsNt2oMw"},"source":["\n","transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n","model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=128)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DG-kW66E6J9"},"source":["# Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"U5IC2aabFFGm"},"source":["### Tokenizing"]},{"cell_type":"code","metadata":{"id":"HgHk__Csygmo"},"source":["# First load the real tokenizer\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Save the loaded tokenizer locally\n","save_path = 'distilbert_base_uncased/'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","tokenizer.save_pretrained(save_path)\n","\n","\n","# Reload it with the huggingface tokenizers library\n","fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\n","fast_tokenizer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggg7LDGHbM8T"},"source":["### Encoding"]},{"cell_type":"code","metadata":{"id":"CAuEIRZ1ySUk"},"source":["def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding(length=maxlen)\n","    all_ids = []\n","    \n","    for i in tqdm(range(0, len(texts), chunk_size)):\n","        text_chunk = texts[i:i+chunk_size].tolist()\n","        encs = tokenizer.encode_batch(text_chunk)\n","        all_ids.extend([enc.ids for enc in encs])\n","    \n","    return np.array(all_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zC-ptcvky8PS"},"source":["x = fast_encode(train.comment_text.astype(str), fast_tokenizer)\n","x_test = fast_encode(test.comment_text.astype(str), fast_tokenizer)\n","y = train.labels.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Xr_gaBE5si9"},"source":["# Preparing datasets\n","\n"]},{"cell_type":"code","metadata":{"id":"mM1PCCgPqEuu"},"source":["BATCH_SIZE=64\n","\n","train_dataset = (\n","    tf.data.Dataset \n","      .from_tensor_slices((x, y))\n","      .repeat()\n","      .shuffle(2048)\n","      .batch(BATCH_SIZE)\n","    # AUTOTUNE prompts the runtime to prepare the next set \n","    # while processing the current one\n","    .prefetch(tf.data.experimental.AUTOTUNE) \n",")\n","\n","test_data = (\n","    tf.data.Dataset# create dataset\n","    .from_tensor_slices(x_test) \n","    .batch(BATCH_SIZE)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BARpyVXx69f8"},"source":["# training "]},{"cell_type":"code","metadata":{"id":"kwxBZGTW27Ow"},"source":["class_weight = {0: 1.,\n","                1: 7.\n","                }\n","\n","\n","\n","train_history = model.fit(\n","                              train_dataset,\n","                              steps_per_epoch=10,\n","                              class_weight=class_weight,\n","                              epochs=10,\n","                             )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqqQPCkaBuNs"},"source":["# Predicting on test set"]},{"cell_type":"code","source":["dataset_name = 'slang'\n","\n","saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n","\n","model.save(saved_model_path, include_optimizer=False)\n","\n"],"metadata":{"id":"oWOZccPEH6PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"id":"EwFkcN87HjM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reloaded_model = tf.saved_model.load(\"/content/slang_bert\")"],"metadata":{"id":"Nl-yixCBooFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjIWaCpSuZ5U"},"source":["test['toxic'] = model.predict(test_data, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DI7A-FWGowB"},"source":["Save it to CSV and load it "]},{"cell_type":"code","metadata":{"id":"XlySBK2CBzMX"},"source":["test.to_csv('test.csv', index=False)\n","data=pd.read_csv('/content/test.csv')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cQNY6T41OJO"},"source":["def replace(toxic):\n","  if toxic >=0.5:\n","    toxic=1\n","  else:\n","    toxic=0\n","  return toxic\n","\n","test['prediction']=test['toxic'].apply(lambda x : replace(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVvGPaHr2aGF"},"source":["test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PbJPJC93IvI"},"source":["test.prediction.value_counts().plot(kind='bar')\n","plt.xlabel('toxic or non-toxic')\n","plt.ylabel('count')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEvwK9X0kOhi"},"source":["text1=test.comment_text[186]\n","text1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXZHF_-CDkDH"},"source":["def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding(length=maxlen)\n","    all_ids = []\n","    \n","    #for i in tqdm(range(0, len(texts), chunk_size)):\n","    #text_chunk = texts[i:i+chunk_size].tolist()\n","    encs = tokenizer.encode_batch(texts)\n","    all_ids.extend([enc.ids for enc in encs])\n","\n","    return np.array(all_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAQpSf9i3923"},"source":["p1=fast_encode([text1], fast_tokenizer, maxlen=512)\n","p1 = model.predict(p1)\n","if (replace(p1) == 0):\n","  print (\"Okay contents\")\n","else:\n","  print (\"Contents not permitted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0ST9ke6kxOj"},"source":["text2=test.comment_text[0]\n","text2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BrJ6uxxCzvW"},"source":["p2=fast_encode([text2], fast_tokenizer, maxlen=512)\n","p2=model.predict(p2)\n","if (replace(p2) == 0):\n","  print (\"Okay contents\")\n","else:\n","  print (\"Contents not permitted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UC_Ro_H_lj8L"},"source":["text3 =[\"Every once in a while, I get the urge. You know what I'm talking about, don't you? The urge for destruction. The urge to hurt, maim, kill. It's quite a thing to experience that urge, to let it wash over you, to give in to it. It's addictive. It's all-consuming. You lose yourself to it. It's quite, quite wonderful. I can feel it, even as I speak, tapping around the edges of my mind, trying to prise me open, slip its fingers in. And it would be so easy to let it happen. But we're all like that, aren't we? We're all barbarians at our core. We're all savage, murderous beasts. I know I am. I'm sure you are. The only difference between us, Mr. Prave, is how loudly we roar. I know I roar very loudly indeed. How about you. Do you think you can match me\"]\n","text3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1sAPCz_pwXH"},"source":["p3=fast_encode(text3, fast_tokenizer, maxlen=512)\n","p3=model.predict(p3)\n","if (replace(p2) == 0):\n","  print (\"Okay contents\")\n","else:\n","  print (\"Contents not permitted\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fast_tokenizer\n","model.save"],"metadata":{"id":"IAJ9V70khy6X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"3gRE-bFTheWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle.dump(model,open('toxic1.pkl','wb'))"],"metadata":{"id":"mXMa-MHgheX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle.dump(fast_tokenizer,open('toxicvec1.pkl','wb'))"],"metadata":{"id":"jzlXfGSlhebj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","dbfile = open(\"/content/toxic1.pkl\", 'rb')     \n","db = pickle.load(dbfile)"],"metadata":{"id":"T6pVphVKhec7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db.summary()"],"metadata":{"id":"Cyo2tUBDELNf"},"execution_count":null,"outputs":[]}]}